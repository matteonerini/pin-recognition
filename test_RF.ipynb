{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mainRF.ipynb","provenance":[],"collapsed_sections":["N4Gh6Ukcrico"],"toc_visible":true,"authorship_tag":"ABX9TyM2EVS82ylbMsS9abnuWIfo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"N4Gh6Ukcrico"},"source":["# Student classification\n","In this notebook we classify students from our dataset."]},{"cell_type":"markdown","metadata":{"id":"TWqZf1NYb87q"},"source":["# Data Processing\n","The txt files are uploaded. Data are processed."]},{"cell_type":"code","metadata":{"id":"amSPm9cerIFi","executionInfo":{"status":"ok","timestamp":1606327702370,"user_tz":-60,"elapsed":1914,"user":{"displayName":"Matteo Nerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimJPbIv_whqXACCc3Iu9kpt4akB1XsrHA0q4NCQ=s64","userId":"06615918241077546947"}}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","url = 'https://raw.githubusercontent.com/MatteoNerini/misc/main/Studente13_mod.txt'\n","names = ['Digit','Acc_X','Acc_Y','Acc_Z','Gir_X','Gir_Y','Gir_Z','Gra_X','Gra_Y','Gra_Z',\n","         'Lin_X','Lin_Y','Lin_Z','Rot_Z','Ori_X','Ori_Y','M']\n","dataset_frame = pd.read_csv(url, names=names)\n","dataset = dataset_frame.values\n","\n","n_class = 10\n","test_size = 0.3\n","\n","n_entries, n_feat = dataset.shape\n","n_feat = n_feat -1\n","\n","X = dataset[:,1:17]\n","y = dataset[:,0]\n","\n","#mini = X.min(axis=0)\n","#maxi = X.max(axis=0)\n","#maxmindiff = maxi - mini\n","#X_scaled = np.copy(X)\n","\n","#for i in range(n_entries):\n","#  for j in range(n_feat):\n","#    X_scaled[i,j] = 2*(X[i,j]-mini[j])/maxmindiff[j]-1\n","\n","# Feature scaling\n","scaler=StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Split into train test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=4, stratify=y)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XPj8YwVmdFHg"},"source":["# Model Generation and Training\n","The ML model is generated and trained."]},{"cell_type":"code","metadata":{"id":"JwKuysPZEf5J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606327706127,"user_tz":-60,"elapsed":1399,"user":{"displayName":"Matteo Nerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimJPbIv_whqXACCc3Iu9kpt4akB1XsrHA0q4NCQ=s64","userId":"06615918241077546947"}},"outputId":"c87de93c-b2fe-4b55-fdb3-7339e86d8266"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","\n","## k-fold cross validation RF\n","#n_splits = 5\n","\n","# Initial configuration of the RF\n","#clf_init = RandomForestClassifier(min_samples_split = 4, min_samples_leaf = 2)\n","\n","# Parameters in the random grid\n","#n_estimators = [int(n) for n in np.linspace(start = 150, stop = 200, num = 6)]\n","#criterion = ['gini', 'entropy']\n","#min_samples_split = [3, 4]\n","#min_samples_leaf = [2, 3]\n","\n","# Create the random grid\n","#random_grid = {'n_estimators': n_estimators,\n","#               'criterion': criterion,\n","#               'min_samples_split': min_samples_split,\n","#               'min_samples_leaf': min_samples_leaf}\n","\n","#clf_random = GridSearchCV(estimator = clf_init, param_grid = random_grid, cv = n_splits, verbose=1, n_jobs = -1)\n","#clf_search = clf_random.fit(X_train, y_train)\n","#print(clf_search.best_params_)\n","\n","## Default RF\n","n_estimators = 200\n","\n","#clf_test = RandomForestClassifier(warm_start=True, oob_score=True)\n","#for i in range(1, n_estimators + 1):\n","#    clf_test.set_params(n_estimators=i)\n","#    clf_test.fit(X_train, y_train)\n","#    print(i, clf_test.oob_score_)\n","\n","clf = RandomForestClassifier(n_estimators=n_estimators, min_samples_split = 4, min_samples_leaf = 2)\n","clf.fit(X_train, y_train)\n","\n","#y_pred_conf = clf.predict(X_test)\n","#conf_mat = confusion_matrix(y_test, y_pred_conf)\n","#print(conf_mat)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=2, min_samples_split=4,\n","                       min_weight_fraction_leaf=0.0, n_estimators=200,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"TV5wJX6HdPTH"},"source":["# Model Testing\n","The ML model is tested and performance is evaluated."]},{"cell_type":"code","metadata":{"id":"SA9Nxyu6dadS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606327721848,"user_tz":-60,"elapsed":623,"user":{"displayName":"Matteo Nerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimJPbIv_whqXACCc3Iu9kpt4akB1XsrHA0q4NCQ=s64","userId":"06615918241077546947"}},"outputId":"74a7c56d-2569-4127-ac06-301cd2e24ea3"},"source":["import heapq\n","\n","print('Perf. on Train: ', clf.score(X_train,y_train))\n","print('Perf. on Test: ', clf.score(X_test,y_test))\n","#print(clf_random.score(X_train,y_train))\n","#print(clf_random.score(X_test,y_test))\n","\n","vec_prob = clf.predict_proba(X_test)\n","#print(vec_prob)\n","\n","maxivec = []\n","for i in range(len(X_test)):\n","    index_i = np.argmax(vec_prob[i,:])\n","    maxivec.append(index_i+1)\n","#print(maxivec)\n","\n","total = 0\n","incorrect = 0\n","incorrect_prob = []\n","for i in range(len(y_test)):\n","    total=total+1\n","    if(maxivec[i] != y_test[i]):\n","        incorrect=incorrect+1\n","        incorrect_prob.append(np.max(vec_prob[i,:]))\n","#print('Perf. obtained with maxivec: ', correct/total)\n","\n","vec_pred = clf.predict(X_test)\n","#print(vec_pred)\n","\n","total1 = 0\n","correct1 = 0\n","for i in range(len(y_test)):\n","    total1=total1+1\n","    if(vec_pred[i] == y_test[i]):\n","        correct1=correct1+1\n","#print('Perf. obtained with vec_pred: ', correct1/total1)\n","\n","\n","# Probability of recognition at second and third choice\n","\n","maxivec2 = []\n","maxivec3 = []\n","index_temp2 = []\n","index_temp3 = []\n","for i in range(len(X_test)):\n","    index_temp2 = heapq.nlargest(2, range(len(vec_prob[i,:])), key=vec_prob[i,:].__getitem__)\n","    index_temp3 = heapq.nlargest(3, range(len(vec_prob[i,:])), key=vec_prob[i,:].__getitem__)\n","    index_i2 = index_temp2[1]\n","    index_i3 = index_temp3[2]\n","    maxivec2.append(index_i2+1)\n","    maxivec3.append(index_i3+1)\n","#print(maxivec2)\n","#print(maxivec3)\n","\n","total2 = 0\n","correct2 = 0\n","for i in range(len(y_test)):\n","    total2=total2+1\n","    if(maxivec2[i] == y_test[i]):\n","        correct2=correct2+1\n","print('Perf. obtained with maxivec2: ', correct2/total2)\n","\n","total3 = 0\n","correct3 = 0\n","for i in range(len(y_test)):\n","    total3=total3+1\n","    if(maxivec3[i] == y_test[i]):\n","        correct3=correct3+1\n","print('Perf. obtained with maxivec3: ', correct3/total3)\n","\n","np.max(incorrect_prob)\n","count = 0\n","for i in incorrect_prob : \n","    if i > 0.80: \n","        count = count + 1\n","print(count)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Perf. on Train:  1.0\n","Perf. on Test:  0.5\n","Perf. obtained with maxivec2:  0.20666666666666667\n","Perf. obtained with maxivec3:  0.1\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ID0V3uOxF06T","executionInfo":{"status":"ok","timestamp":1606004317800,"user_tz":-60,"elapsed":41208,"user":{"displayName":"Matteo Nerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimJPbIv_whqXACCc3Iu9kpt4akB1XsrHA0q4NCQ=s64","userId":"06615918241077546947"}},"outputId":"f1855882-d8b9-47f4-b16b-fe9f78289fe0"},"source":["# Probability of recognition with diversity n_div\n","\n","from itertools import combinations\n","\n","n_div = 6\n","\n","testset = np.column_stack((y_test, X_test))\n","testset_ord = testset[testset[:,0].argsort()]\n","\n","n_test_digit = int(n_entries*test_size/n_class)\n","\n","conf_temp = []\n","conf = []\n","for j in range(n_class):\n","    \n","    testsetj = testset_ord[j*n_test_digit:(j+1)*n_test_digit,:]\n","    testsetj_div_temp = []\n","    testsetj_div = []\n","    \n","    # List all combinations (with n_div items), predict_proba, and sum\n","    comb = combinations(range(0,int(len(testset)/n_class)), n_div)\n","    k = 0\n","    for i in list(comb):\n","        k = k + 1\n","        if k % 20 == 0: #20 or 2000\n","            predict_proba_temp = clf.predict_proba(np.vstack((testsetj[i[0],1:17], testsetj[i[1],1:17],\n","                                                              testsetj[i[2],1:17], testsetj[i[3],1:17],\n","                                                              testsetj[i[4],1:17], testsetj[i[5],1:17])))\n","            #if np.max(predict_proba_temp[0,:]) > 0.5:\n","            #    testsetj_div_temp.append(predict_proba_temp[0,:])\n","            #elif np.max(predict_proba_temp[1,:]) > 0.5:\n","            #    testsetj_div_temp.append(predict_proba_temp[1,:])\n","            #else:\n","            testsetj_div_temp.append(np.sum(predict_proba_temp, axis = 0))\n","    testsetj_div = np.array(testsetj_div_temp)\n","    #print(testsetj_div)\n","    \n","    maxivec_divj = []\n","    for i in range(len(testsetj_div)):\n","        index_i = np.argmax(testsetj_div[i,:])\n","        maxivec_divj.append(index_i+1)\n","    #print(maxivec_divj)\n","    \n","    maxivec_divj_conf = []\n","    for i in range(n_class):\n","        maxivec_divj_conf.append(np.count_nonzero(np.array(maxivec_divj) == i+1))\n","    conf_temp.append(maxivec_divj_conf)\n","\n","conf = np.array(conf_temp)\n","print(conf)\n","print(np.trace(conf))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[250   0   0   0   0   0   0   0   0   0]\n"," [ 23 222   0   5   0   0   0   0   0   0]\n"," [  0   7 205  35   0   3   0   0   0   0]\n"," [ 11  51   2 171  15   0   0   0   0   0]\n"," [  0  59   0  27 144   0  20   0   0   0]\n"," [  0   0   0   0   0 155   0   6  74  15]\n"," [  0   0   9 111   0   0 126   4   0   0]\n"," [  0   0   0   0   0   0   0 177   0  73]\n"," [  0   0   0   0   0   0   0   0 250   0]\n"," [  0   0   0   0   0   0   0   6   0 244]]\n","1944\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUIPvS2XGlM7","executionInfo":{"status":"ok","timestamp":1606002638688,"user_tz":-60,"elapsed":792,"user":{"displayName":"Matteo Nerini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjimJPbIv_whqXACCc3Iu9kpt4akB1XsrHA0q4NCQ=s64","userId":"06615918241077546947"}},"outputId":"f563836e-6707-4433-8fe3-4c8a354eb794"},"source":["first = 0.78\n","second = 0.21\n","\n","p1 = first**4\n","p2 = first**4 + 4*(first**3)*second\n","\n","print(p1, p2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.3701505600000001 0.7687742400000002\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SmpeiPVxyHoL"},"source":["## Data processing imported from Matlab\n","\n","n_class = 10\n","n_entries = len(dataset)\n","en_dec = int(n_entries/n_class)\n","n_feat = len(dataset[0]) - 1\n","n_train = 6*en_dec # 60% dei dati = training set\n","tr_dec = int(n_train/n_class)\n","n_val = 2*en_dec   # 20% dei dati = validation set\n","va_dec = int(n_val/n_class)\n","n_test = 2*en_dec # 20% dei dati = test set\n","te_dec = int(n_test/n_class)\n","\n","# Divisione di datatemp in aXtemp (contenenti righe con label X)\n","# Le matrici aXtemp sono randomizzate\n","a1 = dataset[0:en_dec,:]\n","np.random.shuffle(a1)\n","a10 = dataset[en_dec:2*en_dec,:]\n","np.random.shuffle(a10)\n","a2 = dataset[2*en_dec:3*en_dec,:]\n","np.random.shuffle(a2)\n","a3 = dataset[3*en_dec:4*en_dec,:]\n","np.random.shuffle(a3)\n","a4 = dataset[4*en_dec:5*en_dec,:]\n","np.random.shuffle(a4)\n","a5 = dataset[5*en_dec:6*en_dec,:]\n","np.random.shuffle(a5)\n","a6 = dataset[6*en_dec:7*en_dec,:]\n","np.random.shuffle(a6)\n","a7 = dataset[7*en_dec:8*en_dec,:]\n","np.random.shuffle(a7)\n","a8 = dataset[8*en_dec:9*en_dec,:]\n","np.random.shuffle(a8)\n","a9 = dataset[9*en_dec:n_entries,:]\n","np.random.shuffle(a9)\n","\n","# Data = [n_train; n_val; n_test]\n","data = np.concatenate((a1[0:tr_dec,:],a10[0:tr_dec,:],a2[0:tr_dec,:],a3[0:tr_dec,:],a4[0:tr_dec,:],\n","    a5[0:tr_dec,:],a6[0:tr_dec,:],a7[0:tr_dec,:],a8[0:tr_dec,:],a9[0:tr_dec,:],\n","    a1[tr_dec:tr_dec+va_dec,:],a10[tr_dec:tr_dec+va_dec,:],a2[tr_dec:tr_dec+va_dec,:],a3[tr_dec:tr_dec+va_dec,:],a4[tr_dec:tr_dec+va_dec,:],\n","    a5[tr_dec:tr_dec+va_dec,:],a6[tr_dec:tr_dec+va_dec,:],a7[tr_dec:tr_dec+va_dec,:],a8[tr_dec:tr_dec+va_dec,:],a9[tr_dec:tr_dec+va_dec,:],\n","    a1[tr_dec+va_dec:en_dec,:],a10[tr_dec+va_dec:en_dec,:],a2[tr_dec+va_dec:en_dec,:],a3[tr_dec+va_dec:en_dec,:],a4[tr_dec+va_dec:en_dec,:],\n","    a5[tr_dec+va_dec:en_dec,:],a6[tr_dec+va_dec:en_dec,:],a7[tr_dec+va_dec:en_dec,:],a8[tr_dec+va_dec:en_dec,:],a9[tr_dec+va_dec:en_dec,:]))\n","\n","# Data normalization between [-1,1]\n","mini = data.min(axis=0)\n","maxi = data.max(axis=0)\n","maxmindiff = maxi - mini\n","datanorm = np.copy(data)\n","\n","for i in range(n_entries):\n","  for j in range(n_feat):\n","    datanorm[i,j+1] = 2*(data[i,j+1]-mini[j+1])/maxmindiff[j+1]-1\n","\n","X_train = datanorm[0:n_train,1:17]\n","y_train = datanorm[0:n_train,0]\n","X_val = datanorm[n_train:n_train+n_val,1:17]\n","y_val = datanorm[n_train:n_train+n_val,0]\n","X_test = datanorm[n_train+n_val:n_entries,1:17]\n","y_test = datanorm[n_train+n_val:n_entries,0]"],"execution_count":null,"outputs":[]}]}